{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanushreejaganathan/Sentiment_Analysis_tamil_tulu/blob/main/Tamil_preprocessed_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "# Custom stopwords derived from the dataset\n",
        "custom_stopwords = {\n",
        "    'intha', 'and', 'is', 'k', 'da', 'a', 'music', 'from', 'hit', 'm', 'movie', 'podunga', 'indha', 'இந்த',\n",
        "    'ajith', 'mass', 'படம்', 'vera', 'ngk', 'வாழ்த்துக்கள்', 'here', 'ah', 'vijay', 'release', 'mattum', 'to',\n",
        "    'waiting', 'rajini', 'enna', 'trailer', 'இது', 'star', 'ithu', 'views', 'thalaivar', 'dialogue', 'in',\n",
        "    'திரௌபதி', 'iruku', 'thaan', 'dislike', 'petta', 'tha', 'ellam', 'fans', 'irukku', 'of', 'than', 'love',\n",
        "    'likes', 'sema', 'சாதி', 'வெற்றி', 'ku', 'teaser', 'na', 'super', 'i', 'thala', 'bgm', 'பெற', 'marana',\n",
        "    'nalla', 'thalaiva', 'mela', 'like', 'film', 'yuvan', 'pola', 'suriya', 'anna', 'padam', 'surya', 'am',\n",
        "    'pa', 'la', 'neraya', 'best', 'oru', 'nu', 'semma', 'but', 'pakka', 'all', 'thalapathy', 'ya', 'panna',\n",
        "    'va', 'you', 'level', 'u', 'சார்பாக', 'for', 'tamil', 'ஒரு', 'fan', 'illa', 'sir', 'the', 'தான்'\n",
        "}\n",
        "\n",
        "# Function to get POS tag for lemmatization\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts.\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\n",
        "        'J': wordnet.ADJ,\n",
        "        'N': wordnet.NOUN,\n",
        "        'V': wordnet.VERB,\n",
        "        'R': wordnet.ADV\n",
        "    }\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text, stopwords):\n",
        "    \"\"\"Preprocess text by cleaning, tokenizing, removing stopwords, and stemming/lemmatization.\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        # Lowercase conversion\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "        # Remove HTML tags\n",
        "        text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "        # Remove special characters and numbers\n",
        "        text = re.sub(r'[^a-zA-Z\\u0B80-\\u0BFF\\s]', '', text)  # Tamil Unicode range\n",
        "\n",
        "        # Remove punctuation\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove custom stopwords\n",
        "        tokens = [word for word in tokens if word not in stopwords]\n",
        "\n",
        "        # Remove short words\n",
        "        tokens = [word for word in tokens if len(word) > 2]\n",
        "\n",
        "        # Lemmatization\n",
        "        tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "\n",
        "        # Stemming\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "        # Remove duplicate words\n",
        "        tokens = list(dict.fromkeys(tokens))\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "# Load datasets\n",
        "train_path = '/content/drive/MyDrive/Tam-SA-train.csv'\n",
        "val_path = '/content/drive/MyDrive/Tam-SA-val.csv'\n",
        "test_path = '/content/drive/MyDrive/Tam-SA-test-without-labels.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "val_df = pd.read_csv(val_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "# Replace 'Text' column with preprocessed data\n",
        "train_df['Text'] = train_df['Text'].apply(lambda x: preprocess_text(x, custom_stopwords))\n",
        "val_df['Text'] = val_df['Text'].apply(lambda x: preprocess_text(x, custom_stopwords))\n",
        "test_df['Text'] = test_df['Text'].apply(lambda x: preprocess_text(x, custom_stopwords))\n",
        "\n",
        "# Save processed datasets\n",
        "train_df.to_csv('/content/drive/MyDrive/traincl3.csv', index=False)\n",
        "val_df.to_csv('/content/drive/MyDrive/valcl3.csv', index=False)\n",
        "test_df.to_csv('/content/drive/MyDrive/testcl3.csv', index=False)\n",
        "\n",
        "print(\"Preprocessing complete. Cleaned datasets saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPVo_bVr1Akt",
        "outputId": "2dee086f-8cec-49e9-d790-908784a8336e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete. Cleaned datasets saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGnHj9MM0h95"
      },
      "outputs": [],
      "source": []
    }
  ]
}